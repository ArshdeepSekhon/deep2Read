<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Potential Readings &middot; Deep Learning 2Read
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/deep2Read/public/css/poole.css">
  <link rel="stylesheet" href="/deep2Read/public/css/syntax.css">
  <link rel="stylesheet" href="/deep2Read/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Rubik" >

  <!-- Icons -->
  <link rel="shortcut icon" href="/deep2Read/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/deep2Read/atom.xml">

</head>


  <body class="theme-base-06">

  	<!--<body class="theme-base-08">
    <body class="theme-gradient">
    <body class="theme-base-09">
	<body class="layout-reverse">-0-->

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/deep2Read/">
          Deep Learning 2Read
        </a>
      </h1> <br><br>
      <p class="lead">A List of Deep Learning Papers We Read:</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/deep2Read/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/deep2Read//About/">Course Information</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/deep2Read//Basic2LearnDeep/">Basic Readings</a>
          
        
      
        
          
            <a class="sidebar-nav-item active" href="/deep2Read//Potential2Read/">Potential Readings</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/deep2Read//ReadingsIndexByDate/">Readings ByDate</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/deep2Read//ReadingsIndexByTags/">Readings ByTag</a>
          
        
      
        
      
        
          
        
      
      <br>
      <a class="sidebar-nav-item" href="https://github.com/QData/deep2Read" target="_blank" >Site GitHub</a>
      <a class="sidebar-nav-item" href="http://www.cs.virginia.edu/yanjun/" target="_blank" >UVA Qdata Lab</a>
      <p>&copy;  <a href="https://twitter.com/Qdatalab" data-widget-id="459649185759764482">Tweets by @Qdatalab</a></p>

    </nav>

  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Potential Readings of Deep Learning We Plan to Finish in 2017-Fall</h1>
  <hr />

<p><a name="topPage"></a></p>

<font face="Arial,Helvetica">
<ul>
<li><a href="#Foud">Topic I: Foundations </a> </li>
<li><a href="#Stru">Topic II: DNN with Varying Structures</a> </li>
<li><a href="#App">Topic III: Reliable and Benchmarking and Applications </a> </li>
<li><a href="#Opt">Topic IV: Optimization </a> </li>
<li><a href="#Gen">Topic V: Generative  </a> </li>
<li><a href="#RL">Topic VI: Reinforcement </a> </li>
</ul>
</font>

<hr />

<p><a name="Foud"></a></p>

<h2 id="foundations">Foundations</h2>
<ol>
  <li><a href="https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/">DeepLearningSummerSchool17</a></li>
  <li>Andrew Ng - Nuts and Bolts of Applying Deep Learning : https://www.youtube.com/watch?v=F1ka6a13S9I  :</li>
  <li>Ganguli - Theoretical Neuroscience and Deep Learning DLSS16 http://videolectures.net/deeplearning2016_ganguli_theoretical_neuroscience/</li>
  <li>Ganguli - Theoretical Neuroscience and Deep Learning.pdf DLSS17 https://drive.google.com/file/d/0B6NHiPcsmak1dkZMbzc2YWRuaGM/view</li>
  <li>Sharp Minima Can Generalize For Deep Nets, Laurent Dinh (Univ. Montreal), Razvan Pascanu, Samy Bengio (Google Brain), Yoshua Bengio (Univ. Montreal)</li>
  <li>Automated Curriculum Learning for Neural Networks, Alex Graves, Marc G. Bellemare, Jacob Menick, Koray Kavukcuoglu, Remi Munos</li>
  <li>Learning to learn without gradient descent by gradient descent, Yutian Chen, Matthew Hoffman, Sergio Gomez, Misha Denil, Timothy Lillicrap, Matthew Botvinick , Nando de Freitas</li>
  <li>Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study, Samuel Ritter<em>, David Barrett</em>, Adam Santoro, Matt Botvinick</li>
  <li>Geometry of Neural Network Loss Surfaces via Random Matrix Theory, Jeffrey Pennington, Yasaman Bahri</li>
  <li>On the Expressive Power of Deep Neural Networks, Maithra Raghu, Ben Poole, Surya Ganguli, Jon Kleinberg, Jascha Sohl-Dickstein</li>
  <li>Neuroscience-Inspired Artificial Intelligence, http://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3</li>
  <li>Understanding deep learning requires rethinking generalization, ICLR17</li>
  <li>On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima, ICLR17</li>
  <li>Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes, ICLR17</li>
  <li>Capacity and Trainability in Recurrent Neural Networks, ICLR17</li>
  <li>Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations, ICLR17</li>
  <li>Frustratingly Short Attention Spans in Neural Language Modeling, ICLR17</li>
  <li>Topology and Geometry of Half-Rectified Network Optimization, ICLR17</li>
  <li>Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning, ICLR17</li>
  <li>Adversarial Feature Learning, ICLR17</li>
  <li>Do Deep Convolutional Nets Really Need to be Deep and Convolutional?, ICLR17</li>
  <li>Why Deep Neural Networks for Function Approximation?, ICLR17</li>
  <li>Bengio - Recurrent Neural Networks - DLSS 2017.pdf: https://drive.google.com/file/d/0ByUKRdiCDK7-LXZkM3hVSzFGTkE/view</li>
  <li>On the Expressive Power of Deep Neural Networks, Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, Jascha Sohl-Dickstein ; PMLR 70:2847-2854</li>
  <li>Equivariance Through Parameter-Sharing, Siamak Ravanbakhsh, Jeff Schneider, Barnabás Póczos ; PMLR 70:2892-2901</li>
  <li>Large-Scale Evolution of Image Classifiers, Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, Alexey Kurakin ; PMLR 70:2902-2911</li>
  <li>Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks, Itay Safran, Ohad Shamir ; PMLR 70:2979-2987</li>
  <li>A Closer Look at Memorization in Deep Networks, ICML17</li>
  <li>Dynamic Word Embeddings, ICML17</li>
  <li>Combining Low-Density Separators with CNNs, Yu-Xiong Wang*, Carnegie Mellon University; Martial Hebert, Carnegie Mellon University, NIPS16</li>
  <li>CNNpack: Packing Convolutional Neural Networks in the Frequency Domain, NIPS16</li>
  <li>Residual Networks are Exponential Ensembles of Relatively Shallow Networks, NIPS16</li>
  <li>Dense Associative Memory for Pattern Recognition, NIPS16</li>
  <li>Learning Kernels with Random Features, Aman Sinha*, Stanford University; John Duchi,</li>
  <li>Simple and Efficient Weighted Minwise Hashing, NIPS16</li>
  <li>Reward Augmented Maximum Likelihood for Neural Structured Prediction</li>
  <li>Unimodal Probability Distributions for Deep Ordinal Classification, ICML17</li>
  <li>End-to-End Learning for Structured Prediction Energy Networks, ICML17</li>
  <li>Orthogonal Random Features, NIPS16</li>
  <li>Learning Structured Sparsity in Deep Neural Networks, NIPS16</li>
  <li>Learning the Number of Neurons in Deep Networks, NIPS16</li>
  <li>Quantized Random Projections and Non-Linear Estimation of Cosine Similarity, NIPS16</li>
  <li>An equivalence between high dimensional Bayes optimal inference and M-estimation, NIPS16</li>
  <li>High Dimensional Structured Superposition Models, NIPS16</li>
  <li>Learning Deep Embeddings with Histogram Loss, NIPS16</li>
  <li>Learning values across many orders of magnitude, NIPS16</li>
  <li>Learning Deep Parsimonious Representations, NIPS16</li>
  <li>Efficient High-Order Interaction-Aware Feature Selection Based on Conditional Mutual Information, NIPS16</li>
  <li>A Bayesian method for reducing bias in neural representational similarity analysis, NIPS16</li>
  <li>Richards - Deep_Learning_in_the_Brain.pd https://drive.google.com/file/d/0B2A1tnmq5zQdcFNkWU1vdDJiT00/view and
https://drive.google.com/file/d/0B2A1tnmq5zQdQWU0Skd6TVVQYUE/view?usp=drive_web</li>
</ol>

<hr />

<p><a name="Stru"></a></p>

<h2 id="dnn-with-varying-structures">DNN with Varying Structures</h2>
<ol>
  <li>SCAN: Learning Abstract Hierarchical Compositional Visual Concepts, https://arxiv.org/pdf/1707.03389.pdf</li>
  <li>Krueger - Bayesian Hypernetworks.pdf https://drive.google.com/file/d/0B6NHiPcsmak1RUlucW1RN29oS3M/view?usp=drive_web</li>
  <li>Leblond and Alayrac - SeaRNN.pdf https://drive.google.com/file/d/0B6NHiPcsmak1SDVEaWc0OWtaV0k/view?usp=drive_web</li>
  <li>Sharir - Overlapping Architectures.pdf https://drive.google.com/file/d/0B6NHiPcsmak1ZzVkci1EdVN2YkU/view?usp=drive_web</li>
  <li>Ullrich - Bayesian Compression.pd https://drive.google.com/file/d/0B6NHiPcsmak1WlRUeHFpSW5OZGc/view?usp=drive_web</li>
  <li>Understanding Synthetic Gradients and Decoupled Neural Interfaces, Wojtek Czarnecki, Grzegorz Świrszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, Koray Kavukcuoglu, ICML17</li>
  <li>Video Pixel Networks, Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, Koray Kavukcuoglu</li>
  <li>AdaNet: Adaptive Structural Learning of Artificial Neural Networks, Corinna Cortes, Xavi Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, Scott Yang</li>
  <li>Learning to Generate Long-term Future via Hierarchical Prediction, Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, Honglak Lee</li>
  <li>Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning, Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli</li>
  <li>Latent LSTM Allocation: Joint Clustering and Non-Linear Dynamic Modeling of Sequence Data,  Manzil Zaheer, Amr Ahmed, Alex Smola</li>
  <li>Large-Scale Evolution of Image Classifiers, Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, Alexey Kurakin</li>
  <li>Sequence Modeling via Segmentations, Chong Wang (Microsoft Research) · Yining Wang (CMU) · Po-Sen Huang (Microsoft Research) · Abdelrahman Mohammad (Microsoft) · Dengyong Zhou (Microsoft Research) · Li Deng (Citadel)</li>
  <li>ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices</li>
  <li>Adaptive Neural Networks for Fast Test-Time Prediction</li>
  <li>Making Neural Programming Architectures Generalize via Recursion, ICLR17</li>
  <li>Optimization as a Model for Few-Shot Learning, ICLR17</li>
  <li>Learning End-to-End Goal-Oriented Dialog, ICLR17</li>
  <li>Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, ICLR17</li>
  <li>Nonparametric Neural Networks, ICLR17</li>
  <li>An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax, ICLR17</li>
  <li>Improving Neural Language Models with a Continuous Cache, ICLR17</li>
  <li>Variational Recurrent Adversarial Deep Domain Adaptation, ICLR17</li>
  <li>Soft Weight-Sharing for Neural Network Compression, ICLR17</li>
  <li>Tracking the World State with Recurrent Entity Networks, (Lecun),  ICLR17</li>
  <li>Deep Biaffine Attention for Neural Dependency Parsing, ICLR17</li>
  <li>Learning to Remember Rare Events, ICLR17</li>
  <li>Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks, ICLR17</li>
  <li>Deep Learning with Dynamic Computation Graphs, ICLR17</li>
  <li>Query-Reduction Networks for Question Answering, ICLR17</li>
  <li>Bidirectional Attention Flow for Machine Comprehension, ICLR17</li>
  <li>Dynamic Coattention Networks For Question Answering, ICLR17</li>
  <li>Structured Attention Networks, ICLR17</li>
  <li>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, (Dean), ICLR17</li>
  <li>Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain, ICLR17</li>
  <li>Mollifying Networks, Bengio, ICLR17</li>
  <li>Automatic Rule Extraction from Long Short Term Memory Networks, ICLR17</li>
  <li>Loss-aware Binarization of Deep Networks, ICLR17</li>
  <li>Deep Multi-task Representation Learning: A Tensor Factorisation Approach, ICLR17</li>
  <li>Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music, ICLR17</li>
  <li>Reasoning with Memory Augmented Neural Networks for Language Comprehension, ICLR17</li>
  <li>Semi-Supervised Classification with Graph Convolutional Networks, ICLR17</li>
  <li>Hierarchical Multiscale Recurrent Neural Networks, ICLR17</li>
  <li>AdaNet: Adaptive Structural Learning of Artificial Neural Networks, ICML17</li>
  <li>Language Modeling with Gated Convolutional Networks, ICML17</li>
  <li>Image-to-Markup Generation with Coarse-to-Fine Attention, ICML17</li>
  <li>Input Switched Affine Networks: An RNN Architecture Designed for Interpretability, ICML17</li>
  <li>Differentiable Programs with Neural Libraries, ICML17</li>
  <li>Convolutional Sequence to Sequence Learning, ICML17</li>
  <li>State-Frequency Memory Recurrent Neural Networks, ICML17</li>
  <li>SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization, Juyong Kim, Yookoon Park, Gunhee Kim, Sung Ju Hwang ; PMLR 70:1866-1874</li>
  <li>Deriving Neural Architectures from Sequence and Graph Kernels
Tao Lei, Wengong Jin, Regina Barzilay, Tommi Jaakkola ; PMLR 70:2024-2033</li>
  <li>Delta Networks for Optimized Recurrent Network Computation, Daniel Neil, Jun Haeng Lee, Tobi Delbruck, Shih-Chii Liu ; PMLR 70:2584-2593</li>
  <li>Recurrent Highway Networks, Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutnı́k, Jürgen Schmidhuber ; PMLR 70:4189-4198</li>
  <li>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing, ICML17</li>
  <li>OptNet: Differentiable Optimization as a Layer in Neural Networks, ICML17</li>
  <li>Swapout: Learning an ensemble of deep architectures, Saurabh Singh*, UIUC; Derek Hoiem, UIUC; David Forsyth, UIUC, NIPS16</li>
  <li>Natural-Parameter Networks: A Class of Probabilistic Neural Networks, Hao Wang*, HKUST; Xingjian Shi, ; Dit-Yan Yeung, NIPS16</li>
  <li>Learning What and Where to Draw, NIPS16</li>
  <li>Hierarchical Question-Image Co-Attention for Visual Question Answering, NIPS16</li>
  <li>Proximal Deep Structured Models, NIPS16</li>
  <li>Direct Feedback Alignment Provides Learning In Deep Neural Networks, NIPS16</li>
  <li>Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes, NIPS16</li>
  <li>Matching Networks for One Shot Learning, NIPS16</li>
  <li>Can Active Memory Replace Attention? Łukasz Kaiser*, ; Samy Bengio, NIPS16</li>
  <li>Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences, NIPS16</li>
  <li>Binarized Neural Networks, NIPS16</li>
  <li>Interaction Networks for Learning about Objects, Relations and Physics, NIPS16</li>
  <li>Optimal Architectures in a Solvable Model of Deep Networks, NIPS16</li>
</ol>

<hr />

<p><a name="App"></a></p>
<h2 id="reliable-and-benchmarking-and-applications">Reliable and Benchmarking and Applications</h2>
<ol>
  <li>Conditional Image Generation with Pixel CNN Decoders, NIPS16</li>
  <li>Dhruv - Visual Dialog - RLSS 2017 https://drive.google.com/file/d/0BzUSSMdMszk6RndSbkEzcnRFMGs/view and
https://drive.google.com/file/d/0BzUSSMdMszk6cDVBMlRqLUs3TFk/view</li>
  <li>Input Switched Affine Networks: An RNN Architecture Designed for Interpretability, Jakob Foerster, Justin Gilmer, Jan Chorowski, Jascha Sohl-Dickstein, David Sussillo</li>
  <li>Axiomatic Attribution for Deep Networks, Ankur Taly, Qiqi Yan,,Mukund Sundararajan</li>
  <li>Differentiable Programs with Neural Libraries, Alex L Gaunt, Marc Brockschmidt, Nate Kushman, Daniel Tarlow</li>
  <li>Neural Optimizer Search with Reinforcement Learning, Irwan Bello, Barret Zoph, Vijay Vasudevan, Quoc Le</li>
  <li>Measuring Sample Quality with Kernels, Jackson Gorham (STANFORD) · Lester Mackey (Microsoft Research)</li>
  <li>Learning Continuous Semantic Representations of Symbolic Expressions, ICML17</li>
  <li>Recovery Guarantees for One-hidden-layer Neural Networks, ICML17</li>
  <li>On the State of the Art of Evaluation in Neural Language Models, https://arxiv.org/abs/1707.05589</li>
  <li>End-to-end Optimized Image Compression, ICLR17</li>
  <li>Multi-Agent Cooperation and the Emergence of (Natural) Language, ICLR17</li>
  <li>Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data, ICLR17</li>
  <li>Deep Learning with Differential Privacy,</li>
  <li>Privacy-Preserving Deep Learning, CCS15</li>
  <li>Learning to Query, Reason, and Answer Questions On Ambiguous Texts, ICLR17</li>
  <li>Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy, ICLR17</li>
  <li>Data Noising as Smoothing in Neural Network Language Models (Ng), ICLR17</li>
  <li>A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks, ICLR17</li>
  <li>Visualizing Deep Neural Network Decisions: Prediction Difference Analysis, ICLR17</li>
  <li>On Detecting Adversarial Perturbations, ICLR17</li>
  <li>Delving into Transferable Adversarial Examples and Black-box Attacks, ICLR17</li>
  <li>Parseval Networks: Improving Robustness to Adversarial Examples, ICML17</li>
  <li>iSurvive: An Interpretable, Event-time Prediction Model for mHealth, ICML17</li>
  <li>Being Robust (in High Dimensions) Can Be Practical, ICML17</li>
  <li>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, ICML17</li>
  <li>On Calibration of Modern Neural Networks, ICML17</li>
  <li>Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs, ICML17</li>
  <li>Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation, ICML17</li>
  <li>Analogical Inference for Multi-relational Embeddings,  Hanxiao Liu, Yuexin Wu, Yiming Yang ; PMLR 70:2168-2178</li>
  <li>Deep Transfer Learning with Joint Adaptation Networks, Mingsheng Long, Han Zhu, Jianmin Wang, Michael I. Jordan ; PMLR 70:2208-2217</li>
  <li>Sequence to Better Sequence: Continuous Revision of Combinatorial Structures, Jonas Mueller, David Gifford, Tommi Jaakkola ; PMLR 70:2536-2544</li>
  <li>Meta Networks, Tsendsuren Munkhdalai, Hong Yu ; PMLR 70:2554-2563</li>
  <li>Geometry of Neural Network Loss Surfaces via Random Matrix Theory, Jeffrey Pennington, Yasaman Bahri ; PMLR 70:2798-2806</li>
  <li>Asymmetric Tri-training for Unsupervised Domain Adaptation, Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada ; PMLR 70:2988-2997</li>
  <li>Developing Bug-Free Machine Learning Systems With Formal Mathematics, Daniel Selsam, Percy Liang, David L. Dill ; PMLR 70:3047-3056</li>
  <li>Learning Important Features Through Propagating Activation Differences, Avanti Shrikumar, Peyton Greenside, Anshul Kundaje ; PMLR 70:3145-3153</li>
  <li>High-Dimensional Structured Quantile Regression, ICML17</li>
  <li>Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs, Rakshit Trivedi, Hanjun Dai, Yichen Wang, Le Song ; PMLR 70:3462-3471</li>
  <li>Learning to Generate Long-term Future via Hierarchical Prediction, Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, Honglak Lee ; PMLR 70:3560-3569</li>
  <li>Sequence Modeling via Segmentations, Chong Wang, Yining Wang, Po-Sen Huang, Abdelrahman Mohamed, Dengyong Zhou, Li Deng ; PMLR 70:3674-3683</li>
  <li>A Unified View of Multi-Label Performance Measures, Xi-Zhu Wu, Zhi-Hua Zhou ; PMLR 70:3780-3788</li>
  <li>Convexified Convolutional Neural Networks, Yuchen Zhang, Percy Liang, Martin J. Wainwright ; PMLR 70:4044-4053</li>
  <li>Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin, ICML17</li>
  <li>Learning Transferrable Representations for Unsupervised Domain Adaptation, NIPS16</li>
  <li>Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity, NIPS16</li>
  <li>Unsupervised Domain Adaptation with Residual Transfer Networks, Mingsheng Long*, Tsinghua University; Han Zhu, Tsinghua University; Jianmin Wang, Tsinghua University; Michael Jordan, NIPS16</li>
  <li>Interpretable Distribution Features with Maximum Testing Power, Wittawat Jitkrittum*, Gatsby Unit, UCL; Zoltan Szabo, ; Kacper Chwialkowski, Gatsby Unit, UCL; Arthur Gretton, NIPS16</li>
  <li>Domain Separation Networks, NIPS16</li>
  <li>Multimodal Residual Learning for Visual QA, NIPS16</li>
  <li>Learning feed-forward one-shot learners, NIPS16</li>
  <li>Adversarial Multiclass Classification: A Risk Minimization Perspective, NIPS16</li>
  <li>Generating Images with Perceptual Similarity Metrics based on Deep Networks, NIPS16</li>
  <li>Dialog-based Language Learning, Jason Weston*, NIPS16</li>
  <li>The Robustness of Estimator Composition, NIPS16</li>
  <li>Large Margin Discriminant Dimensionality Reduction in Prediction Space, NIPS16</li>
  <li>Robustness of classifiers: from adversarial to random noise, NIPS16</li>
  <li>Examples are not Enough, Learn to Criticize! Model Criticism for Interpretable Machine Learning, NIPS16</li>
  <li>Blind Attacks on Machine Learners, Alex Beatson*, Princeton University; Zhaoran Wang, Princeton University; Han Liu, NIPS16</li>
  <li>Composing graphical models with neural networks for structured representations and fast inference, NIPS16</li>
  <li>Spatiotemporal Residual Networks for Video Action Recognition, NIPS16</li>
</ol>

<hr />

<p><a name="Opt"></a></p>

<h2 id="optimization">Optimization</h2>
<ol>
  <li>Johnson - Automatic Differentiation.p https://drive.google.com/file/d/0B6NHiPcsmak1ckYxR2hmRGdzdFk/view</li>
  <li>Osborne - Probabilistic numerics for deep learning - DLSS 2017.pdf https://drive.google.com/file/d/0B2A1tnmq5zQdWHBYOFctNi1KdVU/view</li>
  <li>Learned Optimizers that Scale and Generalize, Olga Wichrowska, Niru Maheswaranathan, Matthew Hoffman, Sergio Gomez, Misha Denil, Nando de Freitas, Jascha Sohl-Dickstein</li>
  <li>Learning to learn by gradient descent by gradient descent</li>
  <li>Asynchronous Stochastic Gradient Descent with Delay Compensation,</li>
  <li>How to Escape Saddle Points Efficiently, Chi Jin (UC Berkeley) · Rong Ge (Duke University) · Praneeth Netrapalli (Microsoft Research) · Sham M. Kakade (University of Washington) · Michael Jordan (UC Berkeley)</li>
  <li>Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter</li>
  <li>Batched High-dimensional Bayesian Optimization via Structural Kernel Learning</li>
  <li>Towards Principled Methods for Training Generative Adversarial Networks, ICLR17</li>
  <li>Optimization as a Model for Few-Shot Learning, ICLR17</li>
  <li>Amortised MAP Inference for Image Super-resolution, ICLR17</li>
  <li>Neural Architecture Search with Reinforcement Learning, ICLR17</li>
  <li>Distributed Second-Order Optimization using Kronecker-Factored Approximations, ICLR17</li>
  <li>Mode Regularized Generative Adversarial Networks, ICLR17</li>
  <li>Highway and Residual Networks learn Unrolled Iterative Estimation, ICLR17</li>
  <li>Snapshot Ensembles: Train 1, Get M for Free, ICLR17</li>
  <li>Learning to Optimize, ICLR17</li>
  <li>Recurrent Batch Normalization, ICLR17</li>
  <li>Adversarially Learned Inference, ICLR17</li>
  <li>Reasoning with Memory Augmented Neural Networks for Language Comprehension, ICLR17</li>
  <li>Deep ADMM-Net for Compressive Sensing MRI, NIPS16</li>
  <li>Sharp Minima Can Generalize For Deep Nets, ICML17</li>
  <li>Forward and Reverse Gradient-Based Hyperparameter Optimization, ICML17</li>
  <li>Automated Curriculum Learning for Neural Networks, ICML17</li>
  <li>How to Escape Saddle Points Efficiently, ICML17</li>
  <li>Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs, ICML17</li>
  <li>An overview of gradient optimization algorithms, (https://arxiv.org/abs/1609.04747)</li>
  <li>Learning Deep Architectures via Generalized Whitened Neural Networks, Ping Luo ; PMLR 70:2238-2246</li>
  <li>The Loss Surface of Deep and Wide Neural Networks, Quynh Nguyen, Matthias Hein ; PMLR 70:2603-2612</li>
  <li>Relative Fisher Information and Natural Gradient for Learning Large Modular Models, Ke Sun, Frank Nielsen ; PMLR 70:3289-3298</li>
  <li>meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting, Xu Sun, Xuancheng Ren, Shuming Ma, Houfeng Wang ; PMLR 70:3299-3308</li>
  <li>Axiomatic Attribution for Deep Networks, Mukund Sundararajan, Ankur Taly, Qiqi Yan ; PMLR 70:3319-3328</li>
  <li>Follow the Moving Leader in Deep Learning, Shuai Zheng, James T. Kwok ; PMLR 70:4110-4119</li>
  <li>Oracle Complexity of Second-Order Methods for Finite-Sum Problems, ICML17</li>
  <li>The Shattered Gradients Problem: If resnets are the answer, then what is the question?, ICML17</li>
  <li>Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks, ICML17</li>
  <li>End-to-End Differentiable Adversarial Imitation Learning, ICML17</li>
  <li>Neural Optimizer Search with Reinforcement Learning, ICML17</li>
  <li>Adaptive Neural Networks for Efficient Inference, ICML17</li>
  <li>Practical Gauss-Newton Optimisation for Deep Learning, ICML17</li>
  <li>Deep Tensor Convolution on Multicores, ICML17</li>
  <li>The Generalized Reparameterization Gradient, Francisco Ruiz*, Columbia University; Michalis K. Titsias, ; David Blei, NIPS16</li>
  <li>Attend, Infer, Repeat: Fast Scene Understanding with Generative Models, NIPS16</li>
  <li>Memory-Efficient Backpropagation Through Time, NIPS16</li>
  <li>Professor Forcing: A New Algorithm for Training Recurrent Networks, NIPS16</li>
  <li>Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, NIPS16</li>
</ol>

<hr />

<p><a name="Gen"></a></p>

<h2 id="generative">Generative</h2>
<ol>
  <li>GAN tutorial by Ian Goodfellow (NIPS 2016):  https://arxiv.org/abs/1701.00160  https://www.youtube.com/watch?v=AJVyzd0rqdc</li>
  <li>Goodfellow - Generative Models I - DLSS 2017 https://drive.google.com/file/d/0ByUKRdiCDK7-bTgxTGoxYjQ4NW8/view</li>
  <li>Courville - Generative Models II - DLSS 2017. https://drive.google.com/file/d/0B_wzP_JlVFcKQ21udGpTSkh0aVk/view</li>
  <li>Makhzani and Frey - PixelGAN Autoencoders.pdf https://drive.google.com/file/d/0B6NHiPcsmak1SFdRN2lmS3FnekE/view</li>
  <li>Welling - Graphical Models and Deep Learning.pd  https://drive.google.com/file/d/0B6NHiPcsmak1NHJHdzEySzNNQ0U/view</li>
  <li>Parallel Multiscale Autoregressive Density Estimation, Scott Reed, Aäron van den Oord, Nal Kalchbrenner, Ziyu Wang, Dan Belov, Nando de Freitas</li>
  <li>Count-Based Exploration with Neural Density Models, Georg Ostrovski, Marc Bellemare, Aaron van den Oord, Remi Munos</li>
  <li>Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo, Maithra Raghu, Ben Poole, Surya Ganguli, Jon Kleinberg, Jascha Sohl-Dickstein</li>
  <li>Johnson - Graphical Models and Deep Learning https://drive.google.com/file/d/0B6NHiPcsmak1RmZ3bmtFWUd5bjA/view?usp=drive_web</li>
  <li>Variational Boosting: Iteratively Refining Posterior Approximations, Andrew Miller, Nicholas J Foti, Ryan Adams</li>
  <li>Stochastic Generative Hashing, Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, Le Song, ICML17</li>
  <li>Robust Structured Estimation with Single-Index Models, ICML17</li>
  <li>Learning to Act by Predicting the Future, ICLR17</li>
  <li>Improving Generative Adversarial Networks with Denoising Feature Matching, ICLR17</li>
  <li>Boosted Generative Models, ICLR17</li>
  <li>The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables, ICLR17</li>
  <li>Robust Probabilistic Modeling with Bayesian Data Reweighting, ICML17</li>
  <li>Deep Generative Models for Relational Data with Side Information, ICML17</li>
  <li>Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, Jiwon Kim ; PMLR 70:1857-1865</li>
  <li>Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks, Lars Mescheder, Sebastian Nowozin, Andreas Geiger ; PMLR 70:2391-2400</li>
  <li>McGan: Mean and Covariance Feature Matching GAN, Youssef Mroueh, Tom Sercu, Vaibhava Goel ; PMLR 70:2527-2535</li>
  <li>Parallel Multiscale Autoregressive Density Estimation, Scott Reed, Aäron Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Yutian Chen, Dan Belov, Nando Freitas ; PMLR 70:2912-2921</li>
  <li>Adversarial Feature Matching for Text Generation, Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, Lawrence Carin ; PMLR 70:4006-4015</li>
  <li>Learning Hierarchical Features from Deep Generative Models, Shengjia Zhao, Jiaming Song, Stefano Ermon ; PMLR 70:4091-4099</li>
  <li>Wasserstein Generative Adversarial Networks, ICML17</li>
  <li>Generalization and Equilibrium in Generative Adversarial Nets (GANs), ICML17</li>
  <li>Exponential Family Embeddings, NIPS16</li>
</ol>

<hr />

<p><a name="RL"></a></p>

<h2 id="reinforcement">Reinforcement</h2>
<ol>
  <li>Hasselt - Deep Reinforcement Learning - RLSS 2017.pdf https://drive.google.com/file/d/0BzUSSMdMszk6UE5TbWdZekFXSE0/view?usp=drive_web</li>
  <li>Pineau - RL Basic Concepts - RLSS 2017.pdf https://drive.google.com/file/d/0BzUSSMdMszk6bjl3eU5CVmU0cWs/view
http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/
and http://videolectures.net/deeplearning2016_pineau_advanced_topics/</li>
  <li>Roux - RL in the Industry - RLSS 2017.pdf https://drive.google.com/file/d/0BzUSSMdMszk6bEprTUpCaHRrQ28/view</li>
  <li>Singh - Steps Towards Continual Learning.pdf https://drive.google.com/file/d/0BzUSSMdMszk6YVhFUUNLZnZLSWs/view?usp=drive_web</li>
  <li>Sutton - Temporal-Difference Learning- RLSS 2017.pd https://drive.google.com/file/d/0BzUSSMdMszk6VE9kMkY2SzQzSW8/view?usp=drive_web</li>
  <li>Szepesvari - Theory of RL - RLSS 2017.pdf https://drive.google.com/file/d/0BzUSSMdMszk6U194Ym5jSnZQbGM/view?usp=drive_web</li>
  <li>Thomas - Safe Reinforcement Learning - RLSS 2017.pdf https://drive.google.com/file/d/0BzUSSMdMszk6TDRMRGRaM0dBcHM/view?usp=drive_web</li>
  <li>Minimax Regret Bounds for Reinforcement Learning, Mohammad Gheshlaghi Azar, Ian Osband, Remi Munos</li>
  <li>Why is Posterior Sampling Better than Optimism for Reinforcement Learning? Ian Osband, Benjamin Van Roy</li>
  <li>DARLA: Improving Zero-Shot Transfer in Reinforcement Learning, Irina Higgins<em>, Arka Pal</em>, Andrei Rusu, Loic Matthey, Chris Burgess, Alexander Pritzel, Matt Botvinick, Charles Blundell, Alexander Lerchner</li>
  <li>A Distributional Perspective on Reinforcement Learning, Marc G. Bellemare<em>, Will Dabney</em>, Remi Munos</li>
  <li>A Laplacian Framework for Option Discovery in Reinforcement Learning, Marlos Machado (Univ. Alberta), Marc G. Bellemare, Michael Bowling</li>
  <li>The Predictron: End-to-End Learning and Planning, David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris</li>
  <li>FeUdal Networks for Hierarchical Reinforcement Learning, Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Hees, Max Jaderberg, David Silver, Koray Kavukcuoglu</li>
  <li>Neural Episodic Control, Alex Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell</li>
  <li>Robust Adversarial Reinforcement Learning, Lerrel Pinto, James Davidson, Rahul Sukthankar, Abhinav Gupta</li>
  <li>Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs, Michael Gygli, Mohammad Norouzi, Anelia Angelova</li>
  <li>Distral: Robust Multitask Reinforcement Learning, https://arxiv.org/pdf/1707.04175.pdf</li>
  <li>Reinforcement Learning with Unsupervised Auxiliary Tasks, ICLR17</li>
  <li>Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic, ICLR17</li>
  <li>DARLA: Improving Zero-Shot Transfer in Reinforcement Learning, ICML17</li>
  <li>Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning, Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli ; PMLR 70:2661-2670</li>
  <li>Count-Based Exploration with Neural Density Models, Georg Ostrovski, Marc G. Bellemare, Aäron Oord, Rémi Munos ; PMLR 70:2721-2730</li>
  <li>Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction, Wen Sun, Arun Venkatraman, Geoffrey J. Gordon, Byron Boots, J. Andrew Bagnell ; PMLR 70:3309-3318</li>
</ol>

<hr />

<h2 id="more">More:</h2>
<ol>
  <li><a href="https://openreview.net/group?id=ICLR.cc/2017/conference">ICLR 2017 Papers</a></li>
  <li><a href="https://2017.icml.cc/Conferences/2017/Schedule?type=Poster">ICML 2017 Papers</a></li>
  <li><a href="https://nips.cc/Conferences/2016/AcceptedPapers">NIPS 2017 papers</a></li>
  <li><a href="http://yann.lecun.com/exdb/publis/index.html">Yann Lecun</a></li>
  <li><a href="https://scholar.google.com/citations?user=kukA0LcAAAAJ">Y. Bengio</a></li>
  <li><a href="https://scholar.google.com/citations?user=JicYPdAAAAAJ&amp;hl=en">G. Hinton</a></li>
  <li><a href="https://scholar.google.com/citations?user=gLnCTgIAAAAJ&amp;hl=en&amp;oi=sra">Juergen Schmidhuber</a></li>
</ol>

<hr />

<div style="position: fixed; bottom: 76px; right:10px; width: 128px; height: 236px; background-color: #FFCF79;">
<a style="position: fixed; bottom:130px; right:10px;" href="#Foud">I: Foundations</a>
<a style="position: fixed; bottom:155px; right:10px;" href="#Stru">II: Structures</a>
<a style="position: fixed; bottom:180px; right:10px;" href="#App">III: Apps</a>
<a style="position: fixed; bottom:205px; right:10px;" href="#Opt">IV: Optimiza.</a>
<a style="position: fixed; bottom:230px; right:10px;" href="#Gen">V: Generative</a>
<a style="position: fixed; bottom:255px; right:10px;" href="#RL">VI: RL</a>
<a style="position: fixed; bottom:80px; right:10px;" href="#topPage" title="Back to Top">BackTop</a>
</div>

</div>

    </div>

  </body>
</html>
